`nn.Embedding`本质是一个 **“可学习的词向量矩阵”**，它的工作逻辑不是 “矩阵乘法”，而是 “**按索引提取行向量**”，并自动扩展维度：

- 输入：2D 张量 `(seq_len, batch_size)` → 每个元素是 “词索引”（比如`src[0,0] = 123`，表示 “第 0 个序列、第 0 个样本的第 1 个词，对应词汇表中索引为 123 的词”）。
- 映射规则：对于输入中的每一个词索引，从`embedding`矩阵（30000 行 ×512 列）中提取对应的 “第 [索引] 行”（即一个 512 维的向量）。
- 输出：在输入的 2D 形状基础上，新增一个 “词向量维度” 作为最后一维，最终形状为 `(seq_len, batch_size, embedding_dim)`。
嵌入层是一个大小为(vocab_size,embedding_dim)的矩阵


上面这个过程也是pytorch中采用的，与索引等价的操作是：将输入转换为ont-hot来表示并与嵌入层矩阵相乘。
为了就是节省存储