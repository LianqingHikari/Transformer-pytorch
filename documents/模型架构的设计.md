# 1 Decoder
decoder可以进一步分为三块，自注意力块，交叉注意力块，前馈网络块。三者的顺序可以理解为：
- 先搞清楚自己是怎么回事
- 在了解自己是怎么回事的基础上，才能够知道“什么样的信息能够帮助我”，在这个基础上进行外部查询
- 进一步的特征变换
## 1.1 自注意力块
Transformer中的设计为：自注意力->dropout->残差链接->标准化
### 1.1.1 归一化
```
attn_output1, attn_weights1 = self.self_attn(  
    query=x,  
    key=x,  
    value=x,  
    attn_mask=tgt_mask,  
    padding_mask=tgt_pad_mask  
)  
x = x + self.dropout1(attn_output1)  
x = self.norm1(x)
```
这种方式被称为后归一化。现代的大模型通常采用**前归一化**，具有训练稳定、收敛快等特点。前归一化的代码如下：
```
x_norm = self.norm1(x)  # 先对输入进行层归一化
attn_output1 = self.self_attn(
    query=x_norm,
    key=x_norm,
    value=x_norm,
    attn_mask=tgt_mask,
    padding_mask=tgt_pad_mask
)
x = x + self.dropout1(attn_output1)  # 残差连接
```
### 1.1.2 顺序
dropout放在自注意力之后：
- 注意力权重已经是softmax输出的概率分布
- 在注意力输出上应用dropout，相当于对**整个注意力机制的结果**进行正则化

残差连接放在dropout之后：
- Dropout在残差连接之前，确保添加到原始输入的是一些"被抑制"的特征
- 这增强了模型的鲁棒性，强迫网络不过度依赖任何单个注意力头的结果
## 1.2 交叉注意力块
为什么是encoder的输出作为k和v，而不是作为q？
- **Encoder → 完整源序列编码**：已经看到了整个输入，可以构建完整的信息库(K,V)
- **Decoder → 逐步生成过程**：当前只生成了部分输出，需要基于当前状态去"查询"相关信息(Q)
逻辑上来说，应该用decoder的信息去关联encoder的信息。
## 1.3 前馈块