# 1 分词器

分词器的训练本质是 “从大规模文本中学习合理的**文本拆分规则**和**词汇表**，核心目标是让拆分后的单元（子词 / 单词）既能覆盖高频语义，又能控制词汇表大小。
## 1.1 核心作用

### 1.1.1 搭建 “文本→数字” 的桥梁（基础功能）

所有 NLP 模型本质上只能处理**数字张量（Tensor）**，无法直接理解字符或单词。分词器的核心任务就是完成 “符号→数字” 的映射：

- 第一步：**拆分文本**（分词 / 子词分割）。例如将英文句子 “I love natural language processing” 拆分为单词级 `["I", "love", "natural", "language", "processing"]`，或子词级 `["I", "love", "natural", "language", "process", "ing"]`。
- 第二步：**建立词汇表（Vocabulary）映射**。为拆分后的每个 “基本单元”（单词 / 子词 / 字符）分配唯一的数字 ID（如 `love` → 123，`process` → 456），最终将文本转换为 `[5, 123, 78, 90, 456, 34]` 这样的数字序列，供模型输入。

### 1.1.2 解决 “未登录词（OOV）” 问题（关键价值）

传统的 “单词级分词”（如按空格拆分英文单词、按字拆分中文）存在致命缺陷：遇到**训练集中未出现过的词（OOV，Out-of-Vocabulary）** 时，只能用 “未知标记（`<unk>`）” 替代，导致语义丢失（例如 “ChatGPT” 若不在词汇表中，会被拆成 `<unk>`，模型无法理解其含义）。

而现代分词器（如 BPE、WordPiece）通过**子词（Subword）分割**解决 OOV 问题：

- 原理：子词是 “介于字符和单词之间的单元”（如 “unhappiness” 拆分为 `["un", "happiness"]`，“ChatGPT” 拆分为 `["Chat", "G", "PT"]`）。即使遇到未见过的词，也能拆分为训练过的子词，避免全用 `<unk>` 替代。
- 示例：若词汇表中没有 “自动驾驶”，子词分词器可能拆为 `["自动", "驾驶"]`（中文）或 `["auto", "driving"]`（英文），模型仍能通过子词的语义组合理解整体含义。

### 1.1.3 平衡 “词汇表大小” 与 “语义覆盖度”（效率优化）

词汇表的大小是 NLP 模型的关键权衡点：

- 若词汇表过小（如仅包含 1 万个单词）：会导致大量词被归为 `<unk>`，语义丢失严重。
- 若词汇表过大（如包含 100 万个单词）：会导致模型的 “嵌入层（Embedding Layer）” 参数爆炸（例如嵌入维度为 512 时，参数量 = 1e6×512=5.12e8），训练速度变慢、内存占用激增。

分词器通过**子词策略**平衡这一矛盾：子词单元的 “颗粒度” 介于字符和单词之间，既能用较小的词汇表（如 3 万～5 万）覆盖绝大多数文本（字符级词汇表需 26 个字母 + 符号，覆盖有限；单词级需几十万，太大），又能避免 OOV，兼顾 “覆盖度” 与 “模型效率”。

### 1.1.4 适配模型输入格式（功能扩展）

现代 NLP 模型对输入格式有明确要求，分词器需承担 “格式标准化” 的角色：

- 添加**特殊标记**：如之前代码中自动添加 `<s>`（句子开始）、`</s>`（句子结束）标记，帮助模型识别句子边界；添加 `<pad>`（填充）标记，将不同长度的文本补成相同长度（批量训练必需）。
- 处理**句对任务**：如机器翻译（源句→目标句）、文本匹配，分词器需区分两个句子的 ID（如给目标句 ID 加偏移，避免与源句混淆），对应代码中 `pair="<s> $A </s> $B:1 </s>:1"` 的配置。

## 1.2 分词器的训练

