# 1 计算
直接看例子来说明两者的不同

样本1：[1, 2, 3, 4] 
样本2：[5, 6, 7, 8] 
样本3：[9, 10, 11, 12]

**Batch Normalization（批归一化）的处理方式**
Batch Norm 是 按 “特征维度” 计算统计量（对同一特征的所有样本求均值和方差）：
- 对第一个特征（第一列 [1,5,9]）：均值 = (1+5+9)/3 = 5，方差 ≈ 10.67
- 归一化后：[(1-5)/√10.67, (5-5)/√10.67, (9-5)/√10.67] ≈ [-1.2, 0, 1.2]
- 对第二个特征（第二列 [2,6,10]）：均值 = (2+6+10)/3 = 6，方差 ≈ 10.67
- 归一化后：[(2-6)/√10.67, (6-6)/√10.67, (10-6)/√10.67] ≈ [-1.2, 0, 1.2]
最终 Batch Norm 会让 每个特征在批次内的分布更稳定（不同样本的同一特征被归一化）。

**Layer Normalization（层归一化）的处理方式**
Layer Norm 是 按 “样本维度” 计算统计量（对单个样本的所有特征求均值和方差）：
- 对样本 1（[1,2,3,4]）：均值 = (1+2+3+4)/4 = 2.5，方差 ≈ 1.25
- 归一化后：[(1-2.5)/√1.25, (2-2.5)/√1.25, (3-2.5)/√1.25, (4-2.5)/√1.25] ≈ [-1.34, -0.45, 0.45, 1.34]
- 对样本 2（[5,6,7,8]）：均值 = (5+6+7+8)/4 = 6.5，方差 ≈ 1.25
- 归一化后：[(5-6.5)/√1.25, (6-6.5)/√1.25, (7-6.5)/√1.25, (8-6.5)/√1.25] ≈ [-1.34, -0.45, 0.45, 1.34]

最终 Layer Norm 会让 单个样本内的特征分布更稳定（同一样本的不同特征被归一化）。

# 2 为什么NLP中常用layer_norm
句子长度往往不一致，batch_norm计算会有问题，比如：
样本1：[1, 2, 3, 0] 
样本2：[5, 6, 7, 8] 
样本3：[9, 10, 0, 0]
layer_norm计算的时候则可以把0给忽略掉