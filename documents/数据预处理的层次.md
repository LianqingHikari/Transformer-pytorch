预处理的合理拆分原则是：

- **单样本级、轻量级预处理**（如 Resize、Tokenize、基础数据清洗）：放在`Dataset`的`__getitem__`中，利用`num_workers`并行加速。
- **批次级预处理**（如动态 padding、批次标准化、按批次采样增强等）：必须放在`collate_fn`中，因为依赖整个批次的信息。
- **全量数据预处理**（如全局统计、复杂特征工程）：放在`DataLoader`之外，提前离线处理。
# 1 单样本级、轻量级预处理：放在`Dataset.__getitem__`中
**核心逻辑**：这类操作满足 “样本独立性” 和 “轻量可并行”，放在`__getitem__`中能最大化利用`DataLoader`的多进程加速。
## 1.1 什么是 “单样本级”？

指每个样本的预处理可以独立完成，**不依赖其他样本的信息**。例如：

- 图像：Resize（将单张图缩放到固定尺寸）、Normalize（用全局均值 / 方差标准化单张图的像素值）、随机翻转（单张图的水平 / 垂直翻转）。
- 文本：Tokenize（将单句文本拆分为词 / 字的索引）、 lowercase（单句转小写）。
- 结构化数据：缺失值填充（用全局固定值填充单样本的缺失字段）、特征缩放（单样本的数值特征标准化）。

这些操作只需要 “当前样本” 的信息，不需要知道其他样本的情况，因此可以独立处理。
## 1.2 为什么要放在`__getitem__`中？

`DataLoader`的`num_workers`参数会创建多个子进程，每个子进程负责调用`__getitem__`读取和处理样本。当预处理是 “单样本级” 时，多个子进程可以并行处理不同的样本（比如进程 1 处理样本 A，进程 2 处理样本 B），大幅减少整体预处理耗时。
# 2 批次级预处理：放在`collate_fn`中

**核心逻辑**：这类操作依赖 “整个批次的全局信息”，必须在拿到一批样本后才能处理，因此只能放在`collate_fn`中。
## 2.1 什么是 “批次级”？

指预处理逻辑需要基于 “当前批次内所有样本的特征” 才能执行。例如：

- 动态 Padding（NLP 常见）：文本序列长度不一，为了组成批次张量，需要用 0 填充到 “当前批次中最长序列的长度”（而非固定长度）。如果在`__getitem__`中提前 Padding 到固定长度，会导致短序列填充过多无效值，浪费计算资源。
- 批次级标准化：部分场景中，需要用当前批次的均值 / 方差对样本做标准化（而非全局统计值），这必须在拿到整个批次后计算均值 / 方差。
- 批次级数据增强：如 “混合增强”（Mixup），需要随机选取批次内的两个样本按比例混合，依赖批次内的样本分布。
- 动态采样：根据批次内样本的标签分布调整权重（如解决类别不平衡），需要先统计当前批次的标签比例。
## 2.2 注意点：平衡效率与必要性

`collate_fn`在主进程（或单进程）中执行，批次级预处理如果计算量过大（比如批次内样本数极多，且操作复杂），可能成为瓶颈。因此，只有 “必须依赖批次信息” 的操作才放在这里，其他操作尽量下沉到`__getitem__`中并行处理。

# 3 全量数据预处理：放在`DataLoader`之外离线处理

**核心逻辑**：这类操作依赖 “整个数据集的全局信息”，且计算成本极高，适合离线一次性处理，避免重复消耗资源。
## 3.1 什么是 “全量数据级”？

指预处理需要基于 “所有训练样本的统计特征”，或操作本身计算量极大（远超单样本 / 批次级）。例如：

- 全局统计计算：图像数据集的均值 / 方差（用于后续单样本 Normalize）、文本数据集的词表构建（统计所有文本中的词频，确定词汇表）、结构化数据的全局最值（用于特征缩放的分母）。
- 复杂特征工程：如对文本做预训练词向量（Word2Vec/GloVe），需要用全量文本训练；对图像做特征提取（如用预训练模型提取深层特征），计算量极大。
- 数据格式转换：将原始数据（如 JSON、XML）批量转换为更易加载的格式（如 HDF5、TFRecord），避免训练时重复解析。
## 3.2 为什么要离线处理？

- 避免重复计算：全量预处理通常只需要执行一次（如词表构建、全局均值计算），如果放在`Dataset`或`collate_fn`中，会在每个 epoch 甚至每个批次中重复计算，浪费大量时间。
- 节省内存 / 计算资源：复杂特征工程（如预训练词向量）的结果可以保存为文件，训练时直接加载，无需占用训练过程中的计算资源（尤其是 GPU）。