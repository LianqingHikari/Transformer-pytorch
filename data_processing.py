# data_processing.py
# åŠ¨æ€ç¼–ç æ¨¡å¼ï¼šå–æ•°æ—¶å®æ—¶å¯¹åŸå§‹å¥å­ç¼–ç ï¼Œä¸æå‰ç¼“å­˜
import os
import torch
import random
import pickle  # æ–°å¢ï¼šç”¨äºåºåˆ—åŒ–/ååºåˆ—åŒ–ç¼“å­˜æ•°æ®
from typing import Optional  # æ–°å¢ï¼šç”¨äºå¯é€‰å‚æ•°çš„ç±»å‹æ³¨è§£
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
# ä»tokenizer.pyå¯¼å…¥åˆ†è¯å™¨ç›¸å…³é…ç½®ä¸å‡½æ•°
from tokenizer import SPECIAL_TOKENS, train_bpe_tokenizer, load_bpe_tokenizer

# -------------------------- æ•°æ®é›†æ ¸å¿ƒé…ç½®ï¼ˆä¸è®ºæ–‡ä¸€è‡´ï¼‰ --------------------------
MAX_SEQ_LENGTH = 512  # æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆé¢„ç•™2ä¸ªä½ç½®ç»™<s>/</s>ï¼‰
MIN_SENTENCE_LENGTH = 1  # æœ€å°å¥å­é•¿åº¦ï¼ˆä»…è¿‡æ»¤ç©ºå¥ï¼‰
MAX_LENGTH_RATIO = 2.0  # å¥å¯¹é•¿åº¦æ¯”ä¸Šé™ï¼ˆè¿‡æ»¤ä¸åˆç†ç¿»è¯‘ï¼‰
VALIDATION_SPLIT_RATIO = 0.05  # è®­ç»ƒé›†æ‹†åˆ†éªŒè¯é›†æ¯”ä¾‹


class WMTTranslationDataset(Dataset):
    """é€‚é…WMTæ•°æ®é›†æ ¼å¼çš„ç¿»è¯‘æ•°æ®é›†ç±»ï¼ˆåŠ¨æ€ç¼–ç ï¼šå–æ•°æ—¶å®æ—¶åˆ†è¯ï¼‰"""

    def __init__(self, src_sentences: list, tgt_sentences: list,
                 src_tokenizer, tgt_tokenizer):
        """
        Args:
            src_sentences: æºè¯­è¨€åŸå§‹å¥å­åˆ—è¡¨ï¼ˆæ¸…æ´—åï¼‰
            tgt_sentences: ç›®æ ‡è¯­è¨€åŸå§‹å¥å­åˆ—è¡¨ï¼ˆæ¸…æ´—åï¼Œä¸æºè¯­è¨€ä¸¥æ ¼å¯¹é½ï¼‰
            src_tokenizer: æºè¯­è¨€BPEåˆ†è¯å™¨ï¼ˆæ¥è‡ªtokenizer.pyï¼‰
            tgt_tokenizer: ç›®æ ‡è¯­è¨€BPEåˆ†è¯å™¨ï¼ˆæ¥è‡ªtokenizer.pyï¼‰
        """
        # éªŒè¯å¥å¯¹æ•°é‡ä¸€è‡´æ€§ï¼ˆæ ¸å¿ƒè¦æ±‚ï¼‰
        assert len(src_sentences) == len(tgt_sentences), \
            "æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€å¥å­æ•°é‡ä¸åŒ¹é…"

        self.src_sentences = src_sentences
        self.tgt_sentences = tgt_sentences
        self.src_tokenizer = src_tokenizer
        self.tgt_tokenizer = tgt_tokenizer

    def __len__(self):
        """æ•°æ®é›†æ€»æ ·æœ¬æ•°"""
        return len(self.src_sentences)

    def __getitem__(self, idx):
        """åŠ¨æ€ç¼–ç ï¼šå–æ•°æ—¶å®æ—¶å¯¹åŸå§‹å¥å­åˆ†è¯ç¼–ç ï¼ˆæ ¸å¿ƒæ”¹åŠ¨ï¼‰"""
        # è·å–åŸå§‹å¥å­
        src_sent = self.src_sentences[idx]
        tgt_sent = self.tgt_sentences[idx]

        # å®æ—¶ç¼–ç æºè¯­è¨€ï¼ˆåˆ†è¯å™¨è‡ªåŠ¨æ·»åŠ <s>å’Œ</s>ï¼Œç¬¦åˆè®ºæ–‡æ ¼å¼ï¼‰
        src_encoded = self.src_tokenizer.encode(src_sent)
        src_ids = torch.tensor(src_encoded.ids, dtype=torch.long)

        # å®æ—¶ç¼–ç ç›®æ ‡è¯­è¨€ï¼ˆç”¨äºTeacher Forcingï¼Œä¸è®ºæ–‡ä¸€è‡´ï¼‰
        tgt_encoded = self.tgt_tokenizer.encode(tgt_sent)
        tgt_ids = torch.tensor(tgt_encoded.ids, dtype=torch.long)

        # è¿”å›ç¼–ç ç»“æœ+åºåˆ—é•¿åº¦ï¼ˆä¾›collate_fnæ’åºç”¨ï¼‰
        return {
            'src': src_ids,
            'tgt': tgt_ids,
            'src_len': len(src_ids),
            'tgt_len': len(tgt_ids)
        }


def load_and_clean_parallel_corpus(
    src_path: str,
    tgt_path: str,
    cache_dir: Optional[str] = None  # æ–°å¢ï¼šç¼“å­˜ç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨æºæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼‰
) -> tuple:
    """åŠ è½½å¹¶æ¸…æ´—å¹³è¡Œè¯­æ–™ï¼ˆæ”¯æŒç¼“å­˜ï¼šé¦–æ¬¡å¤„ç†ä¿å­˜ï¼Œåç»­ç›´æ¥åŠ è½½ï¼‰"""
    src_sents = []
    tgt_sents = []

    # -------------------------- æ–°å¢ï¼šç¼“å­˜è·¯å¾„å¤„ç† --------------------------
    # 1. ç¡®å®šç¼“å­˜ç›®å½•ï¼ˆé»˜è®¤ç”¨æºæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œç”¨æˆ·å¯è‡ªå®šä¹‰ï¼‰
    if cache_dir is None:
        cache_dir = os.path.dirname(src_path)  # æºæ–‡ä»¶æ‰€åœ¨ç›®å½•
    os.makedirs(cache_dir, exist_ok=True)  # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨ï¼ˆä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰

    # 2. ç”Ÿæˆå”¯ä¸€ç¼“å­˜æ–‡ä»¶åï¼ˆåŸºäºæº/ç›®æ ‡æ–‡ä»¶çš„æ–‡ä»¶åï¼Œé¿å…ä¸åŒæ–‡ä»¶ç¼“å­˜å†²çªï¼‰
    src_filename = os.path.splitext(os.path.basename(src_path))[0]  # æºæ–‡ä»¶å‰ç¼€ï¼ˆå¦‚"train.en"â†’"train"ï¼‰
    tgt_filename = os.path.splitext(os.path.basename(tgt_path))[0]  # ç›®æ ‡æ–‡ä»¶å‰ç¼€ï¼ˆå¦‚"train.de"â†’"train"ï¼‰
    cache_filename = f"{src_filename}_vs_{tgt_filename}_cleaned.pkl"  # ç¼“å­˜æ–‡ä»¶åï¼ˆå¦‚"train_vs_train_cleaned.pkl"ï¼‰
    cache_path = os.path.join(cache_dir, cache_filename)  # å®Œæ•´ç¼“å­˜è·¯å¾„

    # 3. æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ï¼šå­˜åœ¨åˆ™åŠ è½½ï¼ˆåŠ è½½å¤±è´¥åˆ™é‡æ–°å¤„ç†ï¼‰
    if os.path.exists(cache_path):
        print(f"\n[ç¼“å­˜æœºåˆ¶] å‘ç°ç¼“å­˜æ–‡ä»¶ï¼š{cache_path}")
        try:
            with open(cache_path, "rb") as f:
                src_sents, tgt_sents = pickle.load(f)  # ååºåˆ—åŒ–åŠ è½½ç¼“å­˜
            print(f"[ç¼“å­˜æœºåˆ¶] ç¼“å­˜åŠ è½½æˆåŠŸï¼å…±åŒ…å« {len(src_sents)} ä¸ªæœ‰æ•ˆå¥å¯¹")
            return src_sents, tgt_sents  # ç›´æ¥è¿”å›åŠ è½½çš„ç¼“å­˜æ•°æ®
        except Exception as e:
            print(f"[ç¼“å­˜æœºåˆ¶] ç¼“å­˜æ–‡ä»¶æŸåæˆ–åŠ è½½å¤±è´¥ï¼š{str(e)}ï¼Œå°†é‡æ–°å¤„ç†æ•°æ®")

    # -------------------------- åŸæœ‰é€»è¾‘ï¼šæ•°æ®æ¸…æ´— --------------------------
    # æ£€æŸ¥åŸå§‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(src_path):
        raise FileNotFoundError(f"æºè¯­è¨€æ–‡ä»¶ä¸å­˜åœ¨ï¼š{src_path}")
    if not os.path.exists(tgt_path):
        raise FileNotFoundError(f"ç›®æ ‡è¯­è¨€æ–‡ä»¶ä¸å­˜åœ¨ï¼š{tgt_path}")

    # è®¡ç®—æºæ–‡ä»¶æ€»è¡Œæ•°ï¼ˆç”¨äºtqdmè¿›åº¦æ¡ï¼‰
    def file_line_count(file_path):
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            return sum(1 for _ in f)

    total_lines = file_line_count(src_path)

    # é€è¡Œè¯»å–å¹¶æ¸…æ´—æ•°æ®
    with open(src_path, 'r', encoding='utf-8', errors='ignore') as src_f, \
         open(tgt_path, 'r', encoding='utf-8', errors='ignore') as tgt_f:

        for line_idx, (src_line, tgt_line) in tqdm(
            enumerate(zip(src_f, tgt_f), 1),
            desc=f"[æ•°æ®æ¸…æ´—] å¤„ç† {src_filename} vs {tgt_filename}...",
            total=total_lines
        ):
            src_line = src_line.strip()
            tgt_line = tgt_line.strip()

            # 1. è¿‡æ»¤ç©ºå¥å­
            if not src_line or not tgt_line:
                continue

            # 2. è¿‡æ»¤è¶…é•¿åºåˆ—ï¼ˆé¢„ç•™2ä¸ªä½ç½®ç»™<s>/</s>ï¼‰
            src_token_count = len(src_line.split())  # æŒ‰ç©ºæ ¼ç²—ç»Ÿè®¡ï¼ˆé¿å…æå‰åˆ†è¯ï¼‰
            tgt_token_count = len(tgt_line.split())
            if src_token_count > MAX_SEQ_LENGTH - 2 or tgt_token_count > MAX_SEQ_LENGTH - 2:
                continue

            # 3. è¿‡æ»¤æç«¯é•¿åº¦æ¯”ä¾‹çš„å¥å¯¹
            len_ratio = max(src_token_count / tgt_token_count, tgt_token_count / src_token_count)
            if len_ratio > MAX_LENGTH_RATIO:
                continue

            # ä¿ç•™æ¸…æ´—åçš„åŸå§‹å¥å­
            src_sents.append(src_line)
            tgt_sents.append(tgt_line)

    # -------------------------- æ–°å¢ï¼šä¿å­˜ç¼“å­˜ --------------------------
    print(f"\n[ç¼“å­˜æœºåˆ¶] æ•°æ®æ¸…æ´—å®Œæˆï¼Œå…±ä¿ç•™ {len(src_sents)} ä¸ªæœ‰æ•ˆå¥å¯¹")
    try:
        with open(cache_path, "wb") as f:
            pickle.dump((src_sents, tgt_sents), f)  # åºåˆ—åŒ–ä¿å­˜æ¸…æ´—åçš„æ•°æ®
        print(f"[ç¼“å­˜æœºåˆ¶] ç¼“å­˜å·²ä¿å­˜è‡³ï¼š{cache_path}ï¼ˆä¸‹æ¬¡å¯ç›´æ¥åŠ è½½ï¼‰")
    except Exception as e:
        print(f"[ç¼“å­˜æœºåˆ¶] ç¼“å­˜ä¿å­˜å¤±è´¥ï¼š{str(e)}ï¼ˆä½†æ•°æ®æ¸…æ´—ç»“æœå·²æ­£å¸¸è¿”å›ï¼‰")

    return src_sents, tgt_sents


def split_train_validation(src_train: list, tgt_train: list,
                           split_ratio: float = VALIDATION_SPLIT_RATIO) -> tuple:
    """ä»è®­ç»ƒé›†æ‹†åˆ†éªŒè¯é›†ï¼ˆæ‹†åˆ†åŸå§‹å¥å­åˆ—è¡¨ï¼Œä¸æ¶‰åŠç¼–ç ï¼‰"""
    print(f"\nä»è®­ç»ƒé›†æ‹†åˆ†éªŒè¯é›†ï¼šæ‹†åˆ†æ¯”ä¾‹={split_ratio}")
    # ç»‘å®šæº-ç›®æ ‡å¥å¯¹ï¼Œé¿å…æ‹†åˆ†åé”™ä½
    combined_pairs = list(zip(src_train, tgt_train))
    # å›ºå®šç§å­ç¡®ä¿å¯å¤ç°
    random.seed(42)
    random.shuffle(combined_pairs)

    # è®¡ç®—æ‹†åˆ†ç´¢å¼•
    split_idx = int(len(combined_pairs) * (1 - split_ratio))
    train_pairs = combined_pairs[:split_idx]
    val_pairs = combined_pairs[split_idx:]

    # è¿˜åŸä¸ºå•ç‹¬çš„åŸå§‹å¥å­åˆ—è¡¨
    src_train_split, tgt_train_split = zip(*train_pairs)
    src_val_split, tgt_val_split = zip(*val_pairs)

    print(f"æ‹†åˆ†å®Œæˆï¼šè®­ç»ƒé›†{len(src_train_split)}å¥å¯¹ï¼ŒéªŒè¯é›†{len(src_val_split)}å¥å¯¹")
    return list(src_train_split), list(tgt_train_split), list(src_val_split), list(tgt_val_split)


def collate_fn(batch: list) -> dict:
    """æ‰¹å¤„ç†å‡½æ•°ï¼ˆé€»è¾‘ä¸å˜ï¼Œä»æŒ‰é•¿åº¦æ’åº+å¡«å……ï¼Œç¬¦åˆè®ºæ–‡åŠ¨æ€æ‰¹å¤„ç†ç­–ç•¥ï¼‰"""
    # æŒ‰æºè¯­è¨€åºåˆ—é•¿åº¦é™åºæ’åºï¼ˆä¼˜åŒ–å¡«å……æ•ˆç‡ï¼Œå‡å°‘paddingæ•°é‡ï¼‰
    batch.sort(key=lambda x: x['src_len'], reverse=True)

    # æå–ç¼–ç åçš„åºåˆ—
    src_seqs = [item['src'] for item in batch]
    tgt_seqs = [item['tgt'] for item in batch]

    # å¡«å……è‡³æ‰¹æ¬¡å†…æœ€å¤§é•¿åº¦ï¼ˆåºåˆ—é•¿åº¦åœ¨å‰ï¼Œæ‰¹æ¬¡åœ¨åï¼Œç¬¦åˆTransformerè¾“å…¥æ ¼å¼ï¼‰
    src_padded = torch.nn.utils.rnn.pad_sequence(
        src_seqs, batch_first=False, padding_value=0  # <pad>çš„IDä¸º0ï¼ˆåˆ†è¯å™¨é»˜è®¤é…ç½®ï¼‰
    )
    tgt_padded = torch.nn.utils.rnn.pad_sequence(
        tgt_seqs, batch_first=False, padding_value=0
    )

    # æ„å»ºå¡«å……æ©ç ï¼ˆ1=æœ‰æ•ˆtokenï¼Œ0=paddingï¼Œç”¨äºæ³¨æ„åŠ›å±‚å±è”½æ— æ•ˆtokenï¼‰
    src_mask = (src_padded != 0).float()
    tgt_mask = (tgt_padded != 0).float()

    return {
        'src': src_padded,  # å½¢çŠ¶: (max_src_len, batch_size)
        'tgt': tgt_padded,  # å½¢çŠ¶: (max_tgt_len, batch_size)
        'src_mask': src_mask,
        'tgt_mask': tgt_mask
    }


def get_wmt_dataloaders(data_dir: str, batch_size: int = 32,
                        use_predefined_val: bool = True) -> dict:
    """
    è·å–ç¬¦åˆè®ºæ–‡æ ‡å‡†çš„WMTæ•°æ®é›†åŠ è½½å™¨ï¼ˆåŠ¨æ€ç¼–ç æ¨¡å¼ï¼šå–æ•°æ—¶å®æ—¶åˆ†è¯ï¼Œæ”¯æŒæ•°æ®ç¼“å­˜ï¼‰
    Args:
        data_dir: æ•°æ®é›†æ ¹ç›®å½•ï¼ˆéœ€åŒ…å«train.en/train.deç­‰åŸå§‹æ–‡æœ¬æ–‡ä»¶ï¼‰
        batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤32ï¼Œç¬¦åˆTransformerè®­ç»ƒå¸¸è§„è®¾ç½®ï¼‰
        use_predefined_val: æ˜¯å¦ä½¿ç”¨é¢„å®šä¹‰éªŒè¯é›†ï¼ˆnewstest2013ï¼‰ï¼Œé»˜è®¤True
    Returns:
        dict: åŒ…å«è®­ç»ƒ/éªŒè¯/æµ‹è¯•åŠ è½½å™¨ï¼ŒåŠæº/ç›®æ ‡è¯­è¨€åˆ†è¯å™¨
    """
    # 1. å®šä¹‰æ–‡ä»¶è·¯å¾„ï¼ˆä¸¥æ ¼éµå¾ªWMT2014è‹±å¾·æ•°æ®é›†å‘½åè§„èŒƒï¼‰
    file_paths = {
        'train_src': os.path.join(data_dir, 'train.en'),  # è‹±è¯­è®­ç»ƒé›†ï¼ˆåŸå§‹æ–‡æœ¬ï¼‰
        'train_tgt': os.path.join(data_dir, 'train.de'),  # å¾·è¯­è®­ç»ƒé›†ï¼ˆåŸå§‹æ–‡æœ¬ï¼‰
        'test_src': os.path.join(data_dir, 'newstest2014.en'),  # è‹±è¯­æµ‹è¯•é›†ï¼ˆåŸå§‹æ–‡æœ¬ï¼‰
        'test_tgt': os.path.join(data_dir, 'newstest2014.de'),  # å¾·è¯­æµ‹è¯•é›†ï¼ˆåŸå§‹æ–‡æœ¬ï¼‰
        'val_src': os.path.join(data_dir, 'newstest2013.en'),  # è‹±è¯­éªŒè¯é›†ï¼ˆnewstest2013ï¼‰
        'val_tgt': os.path.join(data_dir, 'newstest2013.de')   # å¾·è¯­éªŒè¯é›†ï¼ˆnewstest2013ï¼‰
    }

    # 2. åŸºç¡€æ ¡éªŒï¼šæ£€æŸ¥æ•°æ®é›†ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_dir):
        raise NotADirectoryError(f"æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨ï¼š{data_dir}ï¼Œè¯·ç¡®è®¤è·¯å¾„æ­£ç¡®")

    # 3. åˆå§‹åŒ–å…³é”®ç›®å½•ï¼ˆåˆ†è¯å™¨ç›®å½•+æ•°æ®ç¼“å­˜ç›®å½•ï¼‰
    # 3.1 åˆ†è¯å™¨ä¿å­˜ç›®å½•ï¼ˆç»Ÿä¸€æ”¾åœ¨æ•°æ®é›†ç›®å½•ä¸‹çš„tokenizerså­ç›®å½•ï¼‰
    tokenizer_dir = os.path.join(data_dir, 'tokenizers')
    os.makedirs(tokenizer_dir, exist_ok=True)  # ç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»º
    # 3.2 æ•°æ®ç¼“å­˜ç›®å½•ï¼ˆç»Ÿä¸€æ”¾åœ¨æ•°æ®é›†ç›®å½•ä¸‹çš„cacheå­ç›®å½•ï¼Œé›†ä¸­ç®¡ç†ç¼“å­˜æ–‡ä»¶ï¼‰
    cache_root = os.path.join(data_dir, 'cache')
    os.makedirs(cache_root, exist_ok=True)  # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨

    # 4. åŠ è½½/è®­ç»ƒåˆ†è¯å™¨ï¼ˆä¼˜å…ˆåŠ è½½å·²æœ‰åˆ†è¯å™¨ï¼Œæ— åˆ™ç”¨è®­ç»ƒé›†è®­ç»ƒï¼Œä¿è¯ç¼–ç ä¸€è‡´æ€§ï¼‰
    # 4.1 æºè¯­è¨€ï¼ˆè‹±è¯­ï¼‰åˆ†è¯å™¨ï¼šè·¯å¾„+åŠ è½½/è®­ç»ƒé€»è¾‘
    src_tokenizer_path = os.path.join(tokenizer_dir, 'src_tokenizer_en.json')
    try:
        src_tokenizer = load_bpe_tokenizer(src_tokenizer_path)
        print(f"âœ… æˆåŠŸåŠ è½½è‹±è¯­åˆ†è¯å™¨ï¼š{src_tokenizer_path}")
    except FileNotFoundError:
        print(f"âŒ æœªæ‰¾åˆ°è‹±è¯­åˆ†è¯å™¨ï¼Œå°†ç”¨è‹±è¯­è®­ç»ƒé›†ï¼ˆ{file_paths['train_src']}ï¼‰è®­ç»ƒ...")
        src_tokenizer = train_bpe_tokenizer(
            data_files=[file_paths['train_src']],  # ä»…ç”¨è‹±è¯­è®­ç»ƒé›†åŸå§‹æ–‡æœ¬è®­ç»ƒ
            save_path=src_tokenizer_path,
            special_tokens=SPECIAL_TOKENS  # ä¼ å…¥ç‰¹æ®Štokené…ç½®ï¼ˆæ¥è‡ªtokenizer.pyï¼‰
        )
        print(f"âœ… è‹±è¯­åˆ†è¯å™¨è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜è‡³ï¼š{src_tokenizer_path}")

    # 4.2 ç›®æ ‡è¯­è¨€ï¼ˆå¾·è¯­ï¼‰åˆ†è¯å™¨ï¼šè·¯å¾„+åŠ è½½/è®­ç»ƒé€»è¾‘
    tgt_tokenizer_path = os.path.join(tokenizer_dir, 'tgt_tokenizer_de.json')
    try:
        tgt_tokenizer = load_bpe_tokenizer(tgt_tokenizer_path)
        print(f"âœ… æˆåŠŸåŠ è½½å¾·è¯­åˆ†è¯å™¨ï¼š{tgt_tokenizer_path}")
    except FileNotFoundError:
        print(f"âŒ æœªæ‰¾åˆ°å¾·è¯­åˆ†è¯å™¨ï¼Œå°†ç”¨å¾·è¯­è®­ç»ƒé›†ï¼ˆ{file_paths['train_tgt']}ï¼‰è®­ç»ƒ...")
        tgt_tokenizer = train_bpe_tokenizer(
            data_files=[file_paths['train_tgt']],  # ä»…ç”¨å¾·è¯­è®­ç»ƒé›†åŸå§‹æ–‡æœ¬è®­ç»ƒ
            save_path=tgt_tokenizer_path,
            special_tokens=SPECIAL_TOKENS  # ç»Ÿä¸€ç‰¹æ®Štokené…ç½®
        )
        print(f"âœ… å¾·è¯­åˆ†è¯å™¨è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜è‡³ï¼š{tgt_tokenizer_path}")

    # 5. å¤„ç†è®­ç»ƒé›†ï¼ˆå¸¦ç¼“å­˜ï¼šä¼˜å…ˆåŠ è½½ç¼“å­˜ï¼Œæ— åˆ™æ¸…æ´—å¹¶ä¿å­˜ç¼“å­˜ï¼‰
    print("\n" + "=" * 60)
    print("ğŸ“¥ å¼€å§‹å¤„ç†è®­ç»ƒé›†ï¼ˆè‹±â†’å¾·ï¼‰...")
    # 5.1 è°ƒç”¨å¸¦ç¼“å­˜çš„è¯­æ–™æ¸…æ´—å‡½æ•°ï¼ˆä¼ å…¥ç¼“å­˜ç›®å½•ï¼Œè‡ªåŠ¨å¤„ç†ç¼“å­˜é€»è¾‘ï¼‰
    src_train_raw, tgt_train_raw = load_and_clean_parallel_corpus(
        src_path=file_paths['train_src'],
        tgt_path=file_paths['train_tgt'],
        cache_dir=cache_root  # å…³é”®ï¼šå¯ç”¨ç¼“å­˜æœºåˆ¶
    )
    # 5.2 åˆ›å»ºåŠ¨æ€ç¼–ç æ•°æ®é›†ï¼ˆå–æ•°æ—¶å®æ—¶åˆ†è¯ï¼Œä¸æå‰ç¼“å­˜ç¼–ç ç»“æœï¼‰
    train_dataset = WMTTranslationDataset(
        src_sentences=src_train_raw,
        tgt_sentences=tgt_train_raw,
        src_tokenizer=src_tokenizer,
        tgt_tokenizer=tgt_tokenizer
    )
    print(f"âœ… è®­ç»ƒé›†åˆ›å»ºå®Œæˆï¼šå…±{len(train_dataset)}ä¸ªæœ‰æ•ˆå¥å¯¹ï¼ˆåŠ¨æ€ç¼–ç æ¨¡å¼ï¼‰")

    # 6. å¤„ç†éªŒè¯é›†ï¼ˆä¸¤ç§æ¨¡å¼ï¼šé¢„å®šä¹‰newstest2013 / ä»è®­ç»ƒé›†æ‹†åˆ†ï¼Œå‡æ”¯æŒç¼“å­˜ï¼‰
    print("\n" + "=" * 60)
    print("ğŸ“¥ å¼€å§‹å¤„ç†éªŒè¯é›†...")
    if use_predefined_val and os.path.exists(file_paths['val_src']) and os.path.exists(file_paths['val_tgt']):
        # 6.1 æ¨¡å¼1ï¼šä½¿ç”¨é¢„å®šä¹‰éªŒè¯é›†ï¼ˆnewstest2013ï¼Œæ¨èï¼Œç¬¦åˆè®ºæ–‡è¯„ä¼°æ ‡å‡†ï¼‰
        print(f"â„¹ï¸ ä½¿ç”¨é¢„å®šä¹‰éªŒè¯é›†ï¼šnewstest2013ï¼ˆè‹±â†’å¾·ï¼‰")
        src_val_raw, tgt_val_raw = load_and_clean_parallel_corpus(
            src_path=file_paths['val_src'],
            tgt_path=file_paths['val_tgt'],
            cache_dir=cache_root  # å¯ç”¨ç¼“å­˜
        )
    else:
        # 6.2 æ¨¡å¼2ï¼šä»è®­ç»ƒé›†æ‹†åˆ†éªŒè¯é›†ï¼ˆæ— é¢„å®šä¹‰éªŒè¯é›†æ—¶é™çº§ä½¿ç”¨ï¼‰
        print(f"â„¹ï¸ æœªæ‰¾åˆ°newstest2013ï¼Œå°†ä»è®­ç»ƒé›†æ‹†åˆ†éªŒè¯é›†ï¼ˆæ‹†åˆ†æ¯”ä¾‹={VALIDATION_SPLIT_RATIO}ï¼‰")
        src_train_raw, tgt_train_raw, src_val_raw, tgt_val_raw = split_train_validation(
            src_train=src_train_raw,
            tgt_train=tgt_train_raw,
            split_ratio=VALIDATION_SPLIT_RATIO
        )
        # æ³¨æ„ï¼šæ‹†åˆ†åè®­ç»ƒé›†åŸå§‹å¥å­å˜åŒ–ï¼Œéœ€é‡æ–°åˆ›å»ºè®­ç»ƒé›†
        train_dataset = WMTTranslationDataset(
            src_sentences=src_train_raw,
            tgt_sentences=tgt_train_raw,
            src_tokenizer=src_tokenizer,
            tgt_tokenizer=tgt_tokenizer
        )
        print(f"â„¹ï¸ æ‹†åˆ†åè®­ç»ƒé›†ï¼š{len(train_dataset)}ä¸ªå¥å¯¹ï¼ŒéªŒè¯é›†ï¼š{len(src_val_raw)}ä¸ªå¥å¯¹")

    # 6.3 åˆ›å»ºéªŒè¯é›†ï¼ˆåŠ¨æ€ç¼–ç æ¨¡å¼ï¼‰
    val_dataset = WMTTranslationDataset(
        src_sentences=src_val_raw,
        tgt_sentences=tgt_val_raw,
        src_tokenizer=src_tokenizer,
        tgt_tokenizer=tgt_tokenizer
    )
    print(f"âœ… éªŒè¯é›†åˆ›å»ºå®Œæˆï¼šå…±{len(val_dataset)}ä¸ªæœ‰æ•ˆå¥å¯¹ï¼ˆåŠ¨æ€ç¼–ç æ¨¡å¼ï¼‰")

    # 7. å¤„ç†æµ‹è¯•é›†ï¼ˆå›ºå®šä½¿ç”¨newstest2014ï¼Œæ”¯æŒç¼“å­˜ï¼‰
    print("\n" + "=" * 60)
    print("ğŸ“¥ å¼€å§‹å¤„ç†æµ‹è¯•é›†ï¼ˆè‹±â†’å¾·ï¼Œnewstest2014ï¼‰...")
    # 7.1 è°ƒç”¨å¸¦ç¼“å­˜çš„è¯­æ–™æ¸…æ´—å‡½æ•°
    src_test_raw, tgt_test_raw = load_and_clean_parallel_corpus(
        src_path=file_paths['test_src'],
        tgt_path=file_paths['test_tgt'],
        cache_dir=cache_root  # å¯ç”¨ç¼“å­˜
    )
    # 7.2 åˆ›å»ºæµ‹è¯•é›†ï¼ˆåŠ¨æ€ç¼–ç æ¨¡å¼ï¼‰
    test_dataset = WMTTranslationDataset(
        src_sentences=src_test_raw,
        tgt_sentences=tgt_test_raw,
        src_tokenizer=src_tokenizer,
        tgt_tokenizer=tgt_tokenizer
    )
    print(f"âœ… æµ‹è¯•é›†åˆ›å»ºå®Œæˆï¼šå…±{len(test_dataset)}ä¸ªæœ‰æ•ˆå¥å¯¹ï¼ˆåŠ¨æ€ç¼–ç æ¨¡å¼ï¼‰")

    # 8. åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆåŠ¨æ€æ‰¹å¤„ç†ï¼šæŒ‰åºåˆ—é•¿åº¦æ’åº+å¡«å……ï¼Œä¼˜åŒ–GPUæ•ˆç‡ï¼‰
    print("\n" + "=" * 60)
    print(f"ğŸš€ å¼€å§‹åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆæ‰¹æ¬¡å¤§å°={batch_size}ï¼‰...")
    # 8.1 è®­ç»ƒé›†åŠ è½½å™¨ï¼ˆshuffle=Trueï¼Œè®­ç»ƒæ—¶æ‰“ä¹±æ•°æ®ï¼‰
    train_loader = DataLoader(
        dataset=train_dataset,
        batch_size=batch_size,
        collate_fn=collate_fn,  # è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°ï¼ˆæ’åº+å¡«å……+æ©ç ï¼‰
        shuffle=True,
        num_workers=4,  # å¤šçº¿ç¨‹åŠ è½½ï¼ˆæ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´ï¼Œå»ºè®®â‰¤CPUæ ¸å¿ƒæ•°ï¼‰
        pin_memory=True,  # é”å®šå†…å­˜ï¼ŒåŠ é€ŸGPUæ•°æ®ä¼ è¾“ï¼ˆéœ€é…åˆGPUä½¿ç”¨ï¼‰
        drop_last=False  # ä¸ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´æ‰¹æ¬¡ï¼ˆé¿å…æ•°æ®æµªè´¹ï¼‰
    )
    # 8.2 éªŒè¯é›†åŠ è½½å™¨ï¼ˆshuffle=Falseï¼Œè¯„ä¼°æ—¶å›ºå®šé¡ºåºï¼‰
    val_loader = DataLoader(
        dataset=val_dataset,
        batch_size=batch_size,
        collate_fn=collate_fn,
        shuffle=False,
        num_workers=4,
        pin_memory=True,
        drop_last=False
    )
    # 8.3 æµ‹è¯•é›†åŠ è½½å™¨ï¼ˆshuffle=Falseï¼Œæµ‹è¯•æ—¶å›ºå®šé¡ºåºï¼‰
    test_loader = DataLoader(
        dataset=test_dataset,
        batch_size=batch_size,
        collate_fn=collate_fn,
        shuffle=False,
        num_workers=4,
        pin_memory=True,
        drop_last=False
    )

    # 9. æ‰“å°åŠ è½½å™¨ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ–¹ä¾¿ç”¨æˆ·ç¡®è®¤æ•°æ®è§„æ¨¡ï¼‰
    print(f"\nğŸ“Š æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼ˆåŠ¨æ€ç¼–ç +ç¼“å­˜æ¨¡å¼ï¼‰ï¼š")
    print(f"- è®­ç»ƒé›†ï¼š{len(train_loader)} ä¸ªæ‰¹æ¬¡ï¼ˆæ€»è®¡ {len(train_dataset)} ä¸ªå¥å¯¹ï¼‰")
    print(f"- éªŒè¯é›†ï¼š{len(val_loader)} ä¸ªæ‰¹æ¬¡ï¼ˆæ€»è®¡ {len(val_dataset)} ä¸ªå¥å¯¹ï¼‰")
    print(f"- æµ‹è¯•é›†ï¼š{len(test_loader)} ä¸ªæ‰¹æ¬¡ï¼ˆæ€»è®¡ {len(test_dataset)} ä¸ªå¥å¯¹ï¼‰")
    print(f"- ç¼“å­˜ç›®å½•ï¼š{cache_root}ï¼ˆä¸‹æ¬¡è¿è¡Œå°†ä¼˜å…ˆåŠ è½½ç¼“å­˜ï¼‰")
    print(f"- åˆ†è¯å™¨ç›®å½•ï¼š{tokenizer_dir}ï¼ˆç¼–ç è§„åˆ™å·²å›ºå®šï¼‰")

    # 10. è¿”å›ç»“æœï¼ˆåŠ è½½å™¨+åˆ†è¯å™¨ï¼Œåˆ†è¯å™¨ç”¨äºåç»­æ¨ç†æ—¶çš„è§£ç ï¼‰
    return {
        'train': train_loader,    # è®­ç»ƒé›†åŠ è½½å™¨
        'val': val_loader,        # éªŒè¯é›†åŠ è½½å™¨
        'test': test_loader,      # æµ‹è¯•é›†åŠ è½½å™¨
        'src_tokenizer': src_tokenizer,  # è‹±è¯­åˆ†è¯å™¨ï¼ˆæºè¯­è¨€ï¼‰
        'tgt_tokenizer': tgt_tokenizer   # å¾·è¯­åˆ†è¯å™¨ï¼ˆç›®æ ‡è¯­è¨€ï¼‰
    }


# ç¤ºä¾‹ç”¨æ³•ï¼šéªŒè¯åŠ¨æ€ç¼–ç æµç¨‹
if __name__ == "__main__":
    # æ›¿æ¢ä¸ºä½ çš„WMT2014è‹±å¾·æ•°æ®é›†ç›®å½•ï¼ˆéœ€åŒ…å«train.en/train.deç­‰åŸå§‹æ–‡æœ¬æ–‡ä»¶ï¼‰
    DATA_DIR = "./dataset/WMT2014EngGer"

    try:
        # è·å–åŠ¨æ€ç¼–ç æ¨¡å¼çš„æ•°æ®åŠ è½½å™¨
        dataloaders = get_wmt_dataloaders(
            data_dir=DATA_DIR,
            batch_size=1,
            use_predefined_val=True  # ä¼˜å…ˆä½¿ç”¨newstest2013ä½œä¸ºéªŒè¯é›†
        )

        # éªŒè¯ç¬¬ä¸€ä¸ªè®­ç»ƒæ‰¹æ¬¡ï¼ˆåŠ¨æ€ç¼–ç ç»“æœï¼‰
        print("\n" + "=" * 50)
        print("éªŒè¯è®­ç»ƒé›†æ‰¹æ¬¡ï¼ˆåŠ¨æ€ç¼–ç æ¨¡å¼ï¼‰ï¼š")
        for batch in dataloaders['train']:
            # æ‰“å°å¼ é‡å½¢çŠ¶ï¼ˆä¸é¢„ç¼–ç æ¨¡å¼ä¸€è‡´ï¼Œç¬¦åˆTransformerè¾“å…¥ï¼‰
            print(f"æºè¯­è¨€åºåˆ—å½¢çŠ¶: {batch['src'].shape} â†’ (max_src_len, batch_size)")
            print(f"ç›®æ ‡è¯­è¨€åºåˆ—å½¢çŠ¶: {batch['tgt'].shape} â†’ (max_tgt_len, batch_size)")
            print(f"æºè¯­è¨€æ©ç å½¢çŠ¶: {batch['src_mask'].shape}ï¼ˆä¸æºåºåˆ—å½¢çŠ¶ä¸€è‡´ï¼‰")

            # è§£ç ç¤ºä¾‹å¥å­ï¼ˆéªŒè¯åŠ¨æ€ç¼–ç æ­£ç¡®æ€§ï¼‰
            src_tokenizer = dataloaders['src_tokenizer']
            tgt_tokenizer = dataloaders['tgt_tokenizer']
            # å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„IDè§£ç ä¸ºæ–‡æœ¬
            src_sample_ids = batch['src'][:, 0].numpy()
            tgt_sample_ids = batch['tgt'][:, 0].numpy()
            src_sample_text = src_tokenizer.decode(src_sample_ids)
            tgt_sample_text = tgt_tokenizer.decode(tgt_sample_ids)

            print(f"\nç¬¬ä¸€ä¸ªæ ·æœ¬ï¼ˆæºè¯­è¨€-è‹±è¯­ï¼‰ï¼š{src_sample_text}")
            print(f"ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼ˆç›®æ ‡è¯­è¨€-å¾·è¯­ï¼‰ï¼š{tgt_sample_text}")
            break  # ä»…éªŒè¯ç¬¬ä¸€ä¸ªæ‰¹æ¬¡

        print("\n" + "=" * 50)
        print("åŠ¨æ€ç¼–ç æ¨¡å¼æ•°æ®å¤„ç†æµç¨‹éªŒè¯å®Œæˆï¼Œç¬¦åˆè®ºæ–‡è§„èŒƒï¼")

    except Exception as e:
        print(f"\næ•°æ®å¤„ç†å¤±è´¥ï¼š{str(e)}")